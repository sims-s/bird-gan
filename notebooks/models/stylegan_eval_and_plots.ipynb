{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "sys.path.append('../../src/')\n",
    "import modeling_utils\n",
    "import models\n",
    "import optimization_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing all the figures and plots and stuff makes github unhappy :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(depth, fade_in, step):\n",
    "    gen = models.StyleGanGenerator(6, noise_size).to(device)\n",
    "    gen.load_state_dict(torch.load('big_run/models/gen_ema_depth_%d_fade_%d_step_%d.pt'%(depth, fade_in, step)))\n",
    "    gen = gen.eval()\n",
    "    return gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_size = 512\n",
    "max_depth = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = models.StyleGanGenerator(max_depth, noise_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = max_depth-1\n",
    "load_fade_in = 100\n",
    "load_step = 638\n",
    "# gen.load_state_dict(torch.load(load_path + 'models/gen_ema_depth_%d_fade_%d_step_%d.pt'%(depth, load_fade_in, load_step)))\n",
    "gen = load_model(depth, load_fade_in, load_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.eval()\n",
    "''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot some random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_plots = 10\n",
    "for _ in range(n_plots):\n",
    "    with torch.no_grad():\n",
    "        modeling_utils.plot_imgs(modeling_utils.sample_gen_images(gen, noise_size, device, depth=max_depth-1, alpha=1))\n",
    "        print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images w/ fixed noise\n",
    "Some good seeds are: 14, 15, 32, 36, 41, 46, 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_name(filename):\n",
    "    parts = filename.split('_')\n",
    "    step = parts[-1].split('.')[0]\n",
    "    fade = parts[-3]\n",
    "    depth = parts[-5]\n",
    "    prefix = filename[:filename.find('_depth')]\n",
    "    return prefix, int(depth), int(fade), int(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_path(d, f, s):\n",
    "    return 'gen_ema_depth_%d_fade_%d_step_%d.pt'%(d,f,s)\n",
    "\n",
    "def get_next_gen_ema(depth, fade, step, n_forward_steps = 5, n_forward_fade=5):\n",
    "    for f in range(n_forward_fade):\n",
    "        for s in range(n_forward_steps):\n",
    "            if f==0 and s==0: continue\n",
    "            check_path = make_path(depth, fade + f, step + s)\n",
    "            if os.path.exists('big_run/models/' + check_path):\n",
    "                return check_path\n",
    "            \n",
    "def next_gen_close_time(depth, fade, step, time_thresh=120):\n",
    "    next_path = get_next_gen_ema(depth, fade, step)\n",
    "    if not next_path:\n",
    "        return False\n",
    "    this_time = os.path.getmtime('big_run/models/' + make_path(depth, fade, step))\n",
    "    other_time = os.path.getmtime('big_run/models/' + next_path)\n",
    "    if abs(other_time - this_time) < time_thresh:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fixed_noise_gif(seed):\n",
    "    fixed_noise = modeling_utils.generate_noise(1, noise_size, device, seed=seed)\n",
    "    torch.manual_seed(seed)\n",
    "    per_channel_noise = torch.randn(fixed_noise.size(0), 2*max_depth, 128, 128).to(device)\n",
    "    \n",
    "    files = os.listdir('big_run/models/')\n",
    "    files.sort(key = lambda x: os.path.getmtime('big_run/models/' + x))\n",
    "    \n",
    "    pbar = tqdm(total = len(files)//5)\n",
    "    fixed_imgs = []\n",
    "    for f in files:\n",
    "        prefix, depth, fade, step = parse_name(f)\n",
    "        if not prefix=='gen_ema' : continue\n",
    "        # During training for depth==5, the last checkpoint was saved twice on accident. So skip the extra checkpoints\n",
    "        if depth==5 and next_gen_close_time(depth, fade, step):\n",
    "            continue\n",
    "        tmp_model = load_model(depth, fade, step)\n",
    "        this_img = tmp_model(fixed_noise, depth, fade/100, per_channel_noise=per_channel_noise)\n",
    "        this_img = modeling_utils.swap_channels_batch(this_img)\n",
    "        this_img = modeling_utils.post_model_process(this_img).squeeze()\n",
    "        fixed_imgs.append(this_img)\n",
    "        pbar.update(1)\n",
    "        \n",
    "    def show_img(img, imobj, ax):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        imobj.set_data(img)\n",
    "        return imobj\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(5,5)\n",
    "    fig.tight_layout()\n",
    "    ax = plt.gca()\n",
    "    imobj = ax.imshow(np.zeros(fixed_imgs[-1].shape))\n",
    "    animate_from_idx = lambda i: show_img(fixed_imgs[i], imobj=imobj, ax=ax)\n",
    "    ani = animation.FuncAnimation(fig, animate_from_idx, init_func=lambda: None, frames=len(fixed_imgs), repeat_delay=5000, interval=50)\n",
    "    vid = HTML(ani.to_jshtml())\n",
    "    \n",
    "    return ani, vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ani_14, vid_14 = make_fixed_noise_gif(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ani_15, vid_15 = make_fixed_noise_gif(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ani_32, vid_32 = make_fixed_noise_gif(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ani_36, vid_36 = make_fixed_noise_gif(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ani_41, vid_41 = make_fixed_noise_gif(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ani_46, vid_46 = make_fixed_noise_gif(46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ani_66, vid_66 = make_fixed_noise_gif(66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid_36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid_41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid_46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid_66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_style_mixing(gen, noise_size, depth, src_seeds, dest_seeds, swap_range, title=None):\n",
    "    def add_img_to_ax(ax, img, dim=False):\n",
    "        if dim:\n",
    "            ax.imshow(img, alpha=.5)\n",
    "        else:\n",
    "            ax.imshow(img)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    \n",
    "    num_cols_minus_1 = len(src_seeds)\n",
    "    num_rows_minus_1 = len(dest_seeds)\n",
    "    \n",
    "    fig, ax = plt.subplots(num_rows_minus_1 + 1, num_cols_minus_1 + 1)\n",
    "    fig.set_size_inches(2*num_rows_minus_1+2, 2*num_cols_minus_1 + 2)\n",
    "#     ax[0,0].set_xticks([])\n",
    "#     ax[0,0].set_yticks([])\n",
    "    ax[0,0].axis(\"off\")\n",
    "    \n",
    "    unique_seeds = list(set(src_seeds + dest_seeds))\n",
    "    torch.manual_seed(0)\n",
    "    per_channel_noise = torch.randn(len(unique_seeds), 2*max_depth, 128, 128).to(device)\n",
    "    \n",
    "    per_channel_noise_src_indexer = torch.tensor([unique_seeds.index(s) for s in src_seeds]).to(device).long()\n",
    "    per_channel_noise_dest_indexer = torch.tensor([unique_seeds.index(s) for s in dest_seeds]).to(device).long()\n",
    "    per_channel_noise_src = per_channel_noise[per_channel_noise_src_indexer]\n",
    "    per_channel_noise_dest = per_channel_noise[per_channel_noise_dest_indexer]\n",
    "    \n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src_latents = torch.cat([modeling_utils.generate_noise(1, noise_size, device, seed=s) for s in src_seeds])\n",
    "        dest_latents = torch.cat([modeling_utils.generate_noise(1, noise_size, device, seed=s) for s in dest_seeds])\n",
    "        src_latents_for_synth = gen.mapping_layers(src_latents)\n",
    "        dest_latents_for_synth = gen.mapping_layers(dest_latents)\n",
    "        \n",
    "        src_images = modeling_utils.swap_channels_batch(gen.synthesis_layers(src_latents_for_synth, depth=depth, alpha=1, per_channel_noise = per_channel_noise_src))\n",
    "        dest_images = modeling_utils.swap_channels_batch(gen.synthesis_layers(dest_latents_for_synth, depth=depth, alpha=1, per_channel_noise = per_channel_noise_dest))\n",
    "        \n",
    "        src_images = modeling_utils.post_model_process(src_images)\n",
    "        dest_images = modeling_utils.post_model_process(dest_images)\n",
    "        \n",
    "        for i in range(1, num_cols_minus_1+1):\n",
    "            add_img_to_ax(ax[0,i], src_images[i-1])\n",
    "            ax[0,i].imshow(src_images[i-1])\n",
    "        for i in range(1, num_rows_minus_1+1):\n",
    "            add_img_to_ax(ax[i,0],  dest_images[i-1])\n",
    "            \n",
    "        for i in range(len(dest_latents_for_synth)):\n",
    "            new_dest_latents_for_synth = dest_latents_for_synth[i].unsqueeze(0).repeat(num_cols_minus_1, 1, 1)\n",
    "            new_dest_latents_for_synth[:, swap_range] = src_latents_for_synth[:, swap_range]\n",
    "            \n",
    "            new_dest_per_channel_noise = per_channel_noise_dest[i].unsqueeze(0).repeat(num_cols_minus_1, 1, 1, 1)\n",
    "            new_dest_per_channel_noise[:, swap_range] = per_channel_noise_src[:, swap_range]\n",
    "            \n",
    "            row_images = modeling_utils.swap_channels_batch(gen.synthesis_layers(new_dest_latents_for_synth, depth=depth, alpha=1, per_channel_noise = new_dest_per_channel_noise))\n",
    "            row_images = modeling_utils.post_model_process(row_images)\n",
    "            for j, img in enumerate(row_images):\n",
    "                add_img_to_ax(ax[i+1,j+1], row_images[j], dest_seeds[i]==src_seeds[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_style_mixing(gen, noise_size, max_depth-1, [2,5,32,33], [2,5,32,33], swap_range = np.arange(4), title='Style Swap Resolutions: 4, 8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_style_mixing(gen, noise_size, max_depth-1, [2,5,32,33], [2,5,32,33], swap_range = np.arange(4)+4, title='Style Swap Resolutions: 16, 32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_style_mixing(gen, noise_size, max_depth-1, [2,5,32,33], [2,5,32,33], swap_range = np.arange(4)+8, title='Style Swap Resolutions: 64, 128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "def show_interpolation(gen, noise_size, depth, seeds, mode='first', steps_per_interp=50, save_path=None, loop=False, **ani_kwargs):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(4,4)\n",
    "    imgs = []\n",
    "    latents = torch.cat([modeling_utils.generate_noise(1, noise_size, device, seed=s) for s in seeds])\n",
    "    \n",
    "    per_channel_noise = torch.randn(1, 2*max_depth, 128, 128).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        max_range = len(latents) if loop else len(latents)-1\n",
    "        pbar = tqdm(total = steps_per_interp*max_range, leave=False)\n",
    "        for i in range(max_range):\n",
    "            start = latents[i]\n",
    "            end_idx = i+1 if i < len(latents)-1 else 0\n",
    "            end = latents[end_idx]\n",
    "            if not mode=='first':\n",
    "                start_latents_for_synth = gen.mapping_layers(start.unsqueeze(0))\n",
    "                end_latents_for_synth = gen.mapping_layers(end.unsqueeze(0))\n",
    "                if not isinstance(mode, torch.Tensor):\n",
    "                    mode = torch.tensor(mode).to(device).long()\n",
    "            for interp_coeff in np.linspace(0,1,steps_per_interp):\n",
    "                if mode=='first':\n",
    "                    model_input = ((1-interp_coeff)*start + interp_coeff*end).unsqueeze(0)\n",
    "                    img = gen(model_input, depth=depth, alpha=1, per_channel_noise=per_channel_noise)\n",
    "                else:\n",
    "                    synth_input = start_latents_for_synth\n",
    "                    synth_input[:,mode] = (1-interp_coeff)*start_latents_for_synth[:,mode] + interp_coeff*end_latents_for_synth[:,mode]\n",
    "                    img = gen.synthesis_layers(synth_input, depth, alpha=1, per_channel_noise=per_channel_noise)\n",
    "                    \n",
    "                img = modeling_utils.swap_channels_batch(img)\n",
    "                img = modeling_utils.post_model_process(img).squeeze()\n",
    "                \n",
    "                imgs.append(img)\n",
    "                pbar.update(1)\n",
    "    pbar.close()\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    imgs = [[plt.imshow(im, animated=True)] for im in imgs]\n",
    "    ani = animation.ArtistAnimation(fig, imgs, **ani_kwargs)\n",
    "    return HTML(ani.to_jshtml()), ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# video_latent, ani_latent = show_interpolation(gen, noise_size, max_depth-1, [14, 15, 36, 41, 46, 66], mode=np.arange(12), steps_per_interp=50, \n",
    "#                          interval=128, blit=True, repeat_delay=100, loop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# video_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_first, ani_first = show_interpolation(gen, noise_size, max_depth-1, [14, 15, 36, 41, 46, 66], mode='first', steps_per_interp=200, \n",
    "#                          interval=33, blit=True, repeat_delay=400, loop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# video_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani_first.save('../../results/latent_interpolation/noise_vector_interpolation.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the FID curve during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df = pd.read_csv('big_run/fid_v2/fid_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('FID during Training')\n",
    "fid_df['fid'].plot()\n",
    "plt.xlabel('Quarter Epoch (x=0: First Epoch @ 128x128 resolution)')\n",
    "plt.ylabel('FID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df[fid_df['fid']==fid_df['fid'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
